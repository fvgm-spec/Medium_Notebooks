{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### BERT language model"
      ],
      "metadata": {
        "id": "LiYWpr-Di-qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing required libraries"
      ],
      "metadata": {
        "id": "PQyO44a-jH0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHRDbyuyb1Ec",
        "outputId": "f33f6904-f535-49af-e6f0-8480a6db080a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing required packages"
      ],
      "metadata": {
        "id": "RSClTzUMjZ_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP libraries\n",
        "import nltk  \n",
        "nltk.download('stopwords')  \n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "#DL libraries\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "import tensorflow as tf\n",
        "\n",
        "#Data manipulation\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "#Other libraries\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgFgyseIcFcG",
        "outputId": "388cd71b-6657-42d6-c735-e6af272a1637"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting data path\n",
        "\n",
        "We will use text from the book \"Moby Dick\" by \"Herman Melville\". This dataset is a public domain dataset from Project Gutenberg that comes installed in the nltk package by default into the folder ` /root/nltk_data/` when this notebook is executed."
      ],
      "metadata": {
        "id": "Nq5CWeuAppup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /root/nltk_data/corpora/gutenberg/melville-moby_dick.txt moby_dick.txt"
      ],
      "metadata": {
        "id": "lu7DHNs2p9GQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and pre-processing data\n",
        "\n",
        "Pre-processing the data involves removing stopwords and punctuation and converting the words into tokens, which is the format that BERT needs. Then we will use the tokenizer that Hugging face provides."
      ],
      "metadata": {
        "id": "V24NFqeMqCQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        " # This loads the list of stop words from english language\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "k2VDuprTqECs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper functions"
      ],
      "metadata": {
        "id": "lk0NT9-2jhc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function removes stopwords if they are present in the line\n",
        "def rem_stops_line(line, words):\n",
        "    if len(line) >1:\n",
        "        return [w for w in line if w not in words]\n",
        "    else:\n",
        "        return line\n",
        "\n",
        "# This function finds the position of the masks in a sentence\n",
        "def find_masks(inp):\n",
        "    return np.where(inp.input_ids.numpy()[0] == 103)[0].tolist()\n",
        "\n",
        "# This function removes stop words for an entire text. \n",
        "# Separate functions make it easier to parallelize if required.\n",
        "def remove_stops(text, words = sw):\n",
        "    return [rem_stops_line(line, words) for line in text]\n",
        "\n",
        "# This function predicts for all the positions of the masks\n",
        "def single_prediction(model, inp, mask_loc):\n",
        "    return model(inp).logits[0].numpy()[mask_loc]\n",
        "\n",
        "def return_prediction(model, query):\n",
        "    # Return a prediction for a single sentence\n",
        "    inp = tokenizer(query,return_tensors='tf')\n",
        "    mask_loc = find_masks(inp)\n",
        "    # Find the Prediction with the highest confidence\n",
        "    predicted_tokens = np.argmax(single_prediction(model, inp, mask_loc),axis=1).tolist()\n",
        "    # Decode the numerical value of the returned ID back to the word \n",
        "    return tokenizer.decode(predicted_tokens)\n",
        "\n",
        "def multiple_preds(model, query_list):\n",
        "    # Return predictions for a list of queries\n",
        "    preds = [f\"{x} -> {return_prediction(model, x).split(' ')}\" for x in query_list]\n",
        "    for i in preds: print(i)"
      ],
      "metadata": {
        "id": "m1XdK6qBfogz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first 1000 sentences for demo purposes\n",
        "with open('/content/moby_dick.txt', 'r') as f:\n",
        "    lines = f.readlines()[:1000]"
      ],
      "metadata": {
        "id": "fpmuWXK_lRhb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use the Helper functions loaded above in order to \n",
        "# remove new lines, convert all to lowercase, remove punctuation \n",
        "# and stop words and tokenize\n",
        "\n",
        "lines = [line.rstrip('\\n').lower() for line in lines]\n",
        "lines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\n",
        "filtered_lines = remove_stops(text = lines, words = sw)\n",
        "\n",
        "inputs = tokenizer(lines,\n",
        "                   max_length=100,\n",
        "                   truncation=True,\n",
        "                   padding='max_length',\n",
        "                   return_tensors='tf')"
      ],
      "metadata": {
        "id": "fwLl_DAElfa_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading The Masked Language Model\n",
        "\n",
        "We use the model from the Transformers library directly. By working with the uncased model, all text will be converted into lowercase."
      ],
      "metadata": {
        "id": "4ZrSYRmlqya4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9r35e7MpYvv",
        "outputId": "11c5ac50-10ea-4020-be1b-12732eb3201f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating The Mask"
      ],
      "metadata": {
        "id": "e-MlN1n_6mI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzOdxOrNqc0K",
        "outputId": "61d4af05-63fa-4d3c-8368-6a38e440dba2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1000, 100), dtype=int32, numpy=\n",
              "array([[  101, 11240,  2100, ...,     0,     0,     0],\n",
              "       [  101,   102,     0, ...,     0,     0,     0],\n",
              "       [  101,   102,     0, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101, 28838,  2098, ...,     0,     0,     0],\n",
              "       [  101,  2061,  2357, ...,     0,     0,     0],\n",
              "       [  101,  2046,  2793, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1000, 100), dtype=int32, numpy=\n",
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1000, 100), dtype=int32, numpy=\n",
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkVI_Wo_64Cp",
        "outputId": "e2432e18-4085-43a7-f140-9c9546a7b052"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name:  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a Mask\n",
        "In a masked language model, such as BERT, creating a mask refers to the process of replacing some of the tokens in the inputs previously defined,  with a special token called the [MASK] token, which in this case we have set as 103. \n",
        "Then during training, a certain percentage of the input tokens are randomly selected and replaced with the [MASK] token. The model is then trained to predict the original token based on the surrounding context."
      ],
      "metadata": {
        "id": "-7QWhsvRgA47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MASK\n",
        "inp_ids = []\n",
        "lbs = []\n",
        "for idx, inp in enumerate(inputs.input_ids.numpy()):\n",
        "    tokens = list(set(range(100)) - \n",
        "                         set(np.where((inp == 101) | (inp == 102) \n",
        "                            | (inp == 0))[0].tolist()))\n",
        "    \n",
        "    # Number of tokens to mask\n",
        "    masked_tokens = 0.15 * len(tokens)\n",
        "    masks = np.random.choice(np.array(tokens), \n",
        "                                     size=int(masked_tokens), \n",
        "                                     replace=False)\n",
        "    \n",
        "    # Store special token and inform model\n",
        "    inp[masks.tolist()] = 103\n",
        "    inp_ids.append(inp)\n",
        "    \n",
        "# Converting the tokens to tensors\n",
        "inp_ids = tf.convert_to_tensor(inp_ids)\n",
        "inputs['input_ids'] = inp_ids"
      ],
      "metadata": {
        "id": "HFuyR8Hr7fWA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyu5FsHZEOwf",
        "outputId": "f2385f1e-0030-4dcc-b0ba-fa591fc4de5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1000, 100), dtype=int32, numpy=\n",
              "array([[  101, 11240,  2100, ...,     0,     0,     0],\n",
              "       [  101,   102,     0, ...,     0,     0,     0],\n",
              "       [  101,   102,     0, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101, 28838,  2098, ...,     0,     0,     0],\n",
              "       [  101,  2061,   103, ...,     0,     0,     0],\n",
              "       [  101,  2046,  2793, ...,     0,     0,     0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_list = [\"And what thing soever [MASK] cometh within the chaos\", \n",
        "              \"Scarcely had we [MASK] two days on the sea\", \n",
        "              \"He visited this [MASK] also with a view of [MASK] horse-whales\"]\n",
        "multiple_preds(model, query_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMN9evOOAZ2c",
        "outputId": "120c902b-1777-4795-8694-a6bd20053400"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And what thing soever [MASK] cometh within the chaos -> ['knows']\n",
            "Scarcely had we [MASK] two days on the sea -> ['spent']\n",
            "He visited this [MASK] also with a view of [MASK] horse-whales -> ['island', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bn_6UXNpAfsy"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}